{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Seven Pillars of Statistical Wisdom in Modern AI/ML\n",
    "\n",
    "**Based on Stephen M. Stigler's \"The Seven Pillars of Statistical Wisdom\"**\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In his brilliant 2016 book, statistician Stephen M. Stigler identified seven fundamental conceptual shifts that formed the foundation of modern statistics. These weren't just mathematical techniques‚Äîthey were revolutionary ways of thinking about data, uncertainty, and inference.\n",
    "\n",
    "What's remarkable is that these same principles, formulated in the 18th and 19th centuries, have become the **structural backbone of modern artificial intelligence and machine learning**. While the computational methods have evolved dramatically, the underlying statistical philosophy remains unchanged.\n",
    "\n",
    "This notebook demonstrates each of Stigler's seven pillars through the lens of contemporary AI/ML, showing both classical statistical implementations and cutting-edge deep learning applications.\n",
    "\n",
    "### The Seven Pillars:\n",
    "\n",
    "1. **Aggregation** - The wisdom of crowds\n",
    "2. **Information** - The square root law of diminishing returns\n",
    "3. **Likelihood** - Probabilistic inference\n",
    "4. **Intercomparison** - Data validates itself\n",
    "5. **Regression** - Returning to the mean\n",
    "6. **Design** - How you collect matters\n",
    "7. **Residual** - Structure in what's left over\n",
    "\n",
    "Let's explore each one with working code examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_regression, make_classification, make_blobs\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.linear_model import SGDClassifier, Ridge, LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pillar 1: AGGREGATION\n",
    "\n",
    "### üéØ Core Concept: The Wisdom of Crowds\n",
    "\n",
    "**Historical Context:** Francis Galton's 1907 discovery at a county fair where 787 people guessed the weight of an ox. The median guess was 1,207 pounds‚Äîthe actual weight was 1,198 pounds. The crowd was nearly perfect, despite most individuals being wildly wrong.\n",
    "\n",
    "**Statistical Insight:** Aggregating many weak or noisy estimates produces a result better than almost any individual estimate. Errors cancel out when they're independent.\n",
    "\n",
    "**Modern ML Translation:** Ensemble learning is everywhere in modern AI:\n",
    "- **Random Forests:** Aggregate many decision trees\n",
    "- **Mixture of Experts (MoE):** Used in GPT-4, Mixtral\n",
    "- **Dropout:** Implicitly trains an exponential ensemble\n",
    "- **Boosting:** Sequential aggregation (addressed in Pillar 7)\n",
    "\n",
    "### Example 1A: Random Forest - Bootstrap Aggregating (Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate non-linear, noisy data\n",
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Single Decision Tree (high variance, low bias)\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "tree_acc = tree.score(X_test, y_test)\n",
    "\n",
    "# Random Forest (aggregation of 100 trees)\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "forest.fit(X_train, y_train)\n",
    "forest_acc = forest.score(X_test, y_test)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Random Forest: Aggregation in Action\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Single Decision Tree Accuracy: {tree_acc:.4f}\")\n",
    "print(f\"Random Forest (100 trees) Accuracy: {forest_acc:.4f}\")\n",
    "print(f\"Improvement from Aggregation: {(forest_acc - tree_acc)*100:.2f}%\")\n",
    "print(\"\\n‚Üí The forest is wiser than any single tree\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1B: Mixture of Experts (MoE)\n",
    "\n",
    "**Modern Context:** Models like Mixtral-8x7B and GPT-4 reportedly use Mixture of Experts architectures. Different \"expert\" sub-networks specialize in different types of inputs, and a gating network decides which experts to activate.\n",
    "\n",
    "**Key Insight:** Not all aggregation is equal-weighted. Sometimes we want specialized experts to handle different regions of the input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create data with distinct clusters\n",
    "X_moe, y_moe = make_blobs(n_samples=600, centers=3, n_features=2, \n",
    "                          cluster_std=1.5, random_state=42)\n",
    "X_train_moe, X_test_moe, y_train_moe, y_test_moe = train_test_split(\n",
    "    X_moe, y_moe, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train three expert models on different subsets (simulating specialization)\n",
    "expert1 = LogisticRegression(random_state=42)\n",
    "expert2 = LogisticRegression(random_state=43)\n",
    "expert3 = LogisticRegression(random_state=44)\n",
    "\n",
    "indices = np.arange(len(X_train_moe))\n",
    "np.random.shuffle(indices)\n",
    "split = len(indices) // 3\n",
    "\n",
    "expert1.fit(X_train_moe[indices[:split]], y_train_moe[indices[:split]])\n",
    "expert2.fit(X_train_moe[indices[split:2*split]], y_train_moe[indices[split:2*split]])\n",
    "expert3.fit(X_train_moe[indices[2*split:]], y_train_moe[indices[2*split:]])\n",
    "\n",
    "# Gating network decides which expert to trust for each input\n",
    "gating = LogisticRegression(multi_class='multinomial', random_state=42)\n",
    "expert_labels = np.zeros(len(X_train_moe), dtype=int)\n",
    "expert_labels[indices[:split]] = 0\n",
    "expert_labels[indices[split:2*split]] = 1\n",
    "expert_labels[indices[2*split:]] = 2\n",
    "gating.fit(X_train_moe, expert_labels)\n",
    "\n",
    "# MoE prediction: weighted combination based on gating network\n",
    "gate_probs = gating.predict_proba(X_test_moe)\n",
    "expert_preds = np.array([\n",
    "    expert1.predict_proba(X_test_moe),\n",
    "    expert2.predict_proba(X_test_moe),\n",
    "    expert3.predict_proba(X_test_moe)\n",
    "])\n",
    "\n",
    "# Weighted average of expert predictions\n",
    "moe_proba = np.zeros_like(expert_preds[0])\n",
    "for i in range(len(X_test_moe)):\n",
    "    for j in range(3):\n",
    "        moe_proba[i] += gate_probs[i, j] * expert_preds[j, i]\n",
    "\n",
    "moe_predictions = np.argmax(moe_proba, axis=1)\n",
    "moe_acc = accuracy_score(y_test_moe, moe_predictions)\n",
    "\n",
    "# Compare to single model\n",
    "single_model = LogisticRegression(random_state=42)\n",
    "single_model.fit(X_train_moe, y_train_moe)\n",
    "single_acc = single_model.score(X_test_moe, y_test_moe)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Mixture of Experts: Specialized Aggregation\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Single Model Accuracy: {single_acc:.4f}\")\n",
    "print(f\"Mixture of Experts Accuracy: {moe_acc:.4f}\")\n",
    "print(\"\\n‚Üí Different experts specialize in different input regions\")\n",
    "print(\"‚Üí Used in modern LLMs like Mixtral-8x7B and reportedly GPT-4\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1C: Dropout as Implicit Ensemble\n",
    "\n",
    "**Key Insight:** Dropout (randomly zeroing neurons during training) isn't just regularization‚Äîit's training an exponential ensemble. With 50 hidden units and dropout rate 0.5, you're training 2^50 ‚âà 10^15 different sub-networks. At test time, you implicitly average over all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SimpleDropoutNet(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=50, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Each forward pass uses a different sub-network\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Prepare data\n",
    "X_drop_train = torch.FloatTensor(X_train)\n",
    "y_drop_train = torch.LongTensor(y_train)\n",
    "X_drop_test = torch.FloatTensor(X_test)\n",
    "y_drop_test = torch.LongTensor(y_test)\n",
    "\n",
    "# Train with dropout\n",
    "net_dropout = SimpleDropoutNet(dropout_rate=0.5)\n",
    "optimizer = optim.Adam(net_dropout.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "net_dropout.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net_dropout(X_drop_train)\n",
    "    loss = criterion(outputs, y_drop_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# At test time, dropout is off - we're averaging over all possible sub-networks\n",
    "net_dropout.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = net_dropout(X_drop_test)\n",
    "    dropout_acc = (test_outputs.argmax(dim=1) == y_drop_test).float().mean().item()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Dropout: Training an Exponential Ensemble\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Network with Dropout Accuracy: {dropout_acc:.4f}\")\n",
    "print(f\"\\nWith 50 hidden units at dropout rate 0.5:\")\n",
    "print(f\"  Number of possible sub-networks: 2^50 ‚âà {2**50:.2e}\")\n",
    "print(\"\\n‚Üí Each training step uses a different random sub-network\")\n",
    "print(\"‚Üí At test time, we implicitly average predictions from all sub-networks\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pillar 2: INFORMATION\n",
    "\n",
    "### üéØ Core Concept: The Square Root Law\n",
    "\n",
    "**Historical Context:** The realization that information accumulates with the square root of sample size, not linearly. To halve your error, you need 4√ó more data. To reduce error by 10√ó, you need 100√ó more data.\n",
    "\n",
    "**Statistical Insight:** Standard error ‚àù 1/‚àöN. This is why polling 1,000 people gives nearly as much information as polling 10,000.\n",
    "\n",
    "**Modern ML Translation:**\n",
    "- **Neural Scaling Laws:** The Chinchilla paper and others show that LLM performance follows power laws\n",
    "- **Data Efficiency:** Modern focus on getting more information from less data\n",
    "- **Active Learning:** Strategically selecting which examples to label\n",
    "- **Few-Shot Learning:** Foundation models extract maximal information from minimal examples\n",
    "\n",
    "### Example 2A: Classical Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate a large dataset\n",
    "X_info, y_info = make_classification(n_samples=5000, n_features=20, \n",
    "                                      n_informative=10, random_state=42)\n",
    "\n",
    "data_sizes = [50, 100, 200, 400, 800, 1600, 3200]\n",
    "accuracies = []\n",
    "marginal_gains = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Information Scaling: The ‚àöN Law\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Training Size':<15} | {'Accuracy':<10} | {'Gain from 2x Data':<20}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "prev_acc = None\n",
    "for n in data_sizes:\n",
    "    X_sub, _, y_sub, _ = train_test_split(\n",
    "        X_info, y_info, train_size=n, stratify=y_info, random_state=42\n",
    "    )\n",
    "    \n",
    "    model = SGDClassifier(loss='log_loss', max_iter=1000, tol=1e-3, random_state=42)\n",
    "    model.fit(X_sub, y_sub)\n",
    "    acc = model.score(X_info[3200:], y_info[3200:])  # Fixed test set\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "    if prev_acc is not None:\n",
    "        gain = (acc - prev_acc) * 100\n",
    "        marginal_gains.append(gain)\n",
    "        print(f\"{n:<15} | {acc:.4f}    | +{gain:.3f}%\")\n",
    "    else:\n",
    "        print(f\"{n:<15} | {acc:.4f}    | baseline\")\n",
    "    prev_acc = acc\n",
    "\n",
    "print(\"\\n‚Üí Notice diminishing returns:\")\n",
    "print(f\"   50‚Üí100 (2x data): +{marginal_gains[0]:.3f}%\")\n",
    "print(f\"   1600‚Üí3200 (2x data): +{marginal_gains[-1]:.3f}%\")\n",
    "print(\"\\n‚Üí This is the ‚àöN law in action!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the scaling law\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(data_sizes, accuracies, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Training Data Size (N)', fontsize=12)\n",
    "plt.ylabel('Test Accuracy', fontsize=12)\n",
    "plt.title('Accuracy vs. Data Size\\n(Diminishing Returns)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(data_sizes[1:], marginal_gains, 'ro-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Training Data Size (N)', fontsize=12)\n",
    "plt.ylabel('Marginal Gain (%)', fontsize=12)\n",
    "plt.title('Marginal Gains from Doubling Data\\n(Decreasing Returns)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Modern Implication: This is why frontier AI labs need exponentially\")\n",
    "print(\"   more data and compute for incremental improvements in LLM performance.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2B: Neural Scaling Laws\n",
    "\n",
    "**Modern Context:** The Chinchilla paper (2022) revealed that most LLMs were undertrained. The relationship between model size, dataset size, and performance follows strict power laws:\n",
    "\n",
    "$$L(N) \\approx \\left(\\frac{N_c}{N}\\right)^\\alpha$$\n",
    "\n",
    "Where:\n",
    "- L is the loss\n",
    "- N is parameters/data/compute\n",
    "- Œ± is typically around 0.05-0.1\n",
    "- N_c is a constant\n",
    "\n",
    "This means that to improve loss by 2√ó, you need roughly 10√ó more resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Simulate a scaling law\n",
    "model_sizes = np.array([1e6, 1e7, 1e8, 1e9, 1e10, 1e11])  # 1M to 100B parameters\n",
    "alpha = 0.076  # Typical exponent from Chinchilla paper\n",
    "N_c = 1e13\n",
    "losses = (N_c / model_sizes) ** alpha\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(model_sizes, losses, 'b-', linewidth=3, label='L(N) ~ (N_c/N)^Œ±')\n",
    "plt.scatter(model_sizes, losses, s=100, c='red', zorder=5)\n",
    "\n",
    "# Annotate some famous models (approximate sizes)\n",
    "famous_models = {\n",
    "    'BERT': (1.1e8, 1.8),\n",
    "    'GPT-2': (1.5e9, 1.2),\n",
    "    'GPT-3': (1.75e11, 0.7)\n",
    "}\n",
    "\n",
    "for name, (size, loss_approx) in famous_models.items():\n",
    "    plt.annotate(name, xy=(size, loss_approx), xytext=(10, 10),\n",
    "                textcoords='offset points', fontsize=11,\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "plt.xlabel('Model Size (Parameters)', fontsize=13)\n",
    "plt.ylabel('Loss', fontsize=13)\n",
    "plt.title('Neural Scaling Laws: Power Law Relationship\\nBetween Model Size and Performance', fontsize=14)\n",
    "plt.grid(True, alpha=0.3, which='both')\n",
    "plt.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Neural Scaling Laws\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTo improve loss by 2√ó, you need roughly 10√ó more parameters/data/compute\")\n",
    "print(\"\\nKey Papers:\")\n",
    "print(\"  ‚Ä¢ Kaplan et al. (2020): 'Scaling Laws for Neural Language Models'\")\n",
    "print(\"  ‚Ä¢ Hoffmann et al. (2022): 'Training Compute-Optimal LLMs' (Chinchilla)\")\n",
    "print(\"\\n‚Üí This explains why GPT-4 required orders of magnitude more resources than GPT-3\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pillar 3: LIKELIHOOD\n",
    "\n",
    "### üéØ Core Concept: Probabilistic Inference\n",
    "\n",
    "**Historical Context:** Thomas Bayes and Pierre-Simon Laplace developed the mathematics of inverse probability. Instead of asking \"What's the probability of this data given a hypothesis?\", we ask \"What's the probability of this hypothesis given the data?\"\n",
    "\n",
    "**Statistical Insight:** We rarely know absolute truth. Instead, we find which hypothesis is **most likely** given our observations. Maximum Likelihood Estimation (MLE) is the cornerstone of modern statistics.\n",
    "\n",
    "**Modern ML Translation:**\n",
    "- **Loss Functions:** Cross-entropy loss = negative log likelihood\n",
    "- **Training = MLE:** Adjusting weights to maximize likelihood of observed data\n",
    "- **Softmax:** Converts logits to probability distributions\n",
    "- **Contrastive Learning:** Maximizing likelihood of correct pairs vs incorrect pairs\n",
    "\n",
    "### Example 3A: Cross-Entropy as Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Likelihood: The Foundation of Neural Network Training\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Simple example: 5 samples, 3 classes\n",
    "torch.manual_seed(42)\n",
    "inputs = torch.randn(5, 4)  # 5 samples, 4 features\n",
    "targets = torch.tensor([0, 2, 1, 0, 2])  # True class indices\n",
    "\n",
    "# Model: simple linear layer\n",
    "logits = inputs @ torch.randn(4, 3)  # Project to 3 classes\n",
    "\n",
    "print(\"\\nRaw logits (unnormalized scores):\")\n",
    "print(logits)\n",
    "\n",
    "# Convert to probabilities using softmax\n",
    "probabilities = F.softmax(logits, dim=1)\n",
    "print(\"\\nProbabilities (after softmax):\")\n",
    "print(probabilities)\n",
    "print(\"\\nNote: Each row sums to 1.0 (valid probability distribution)\")\n",
    "\n",
    "# Cross-Entropy Loss = Negative Log Likelihood\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(logits, targets)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Cross-Entropy Loss (Negative Log Likelihood): {loss.item():.4f}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Let's break down what this means\n",
    "print(\"\\nWhat the loss means:\")\n",
    "for i in range(len(targets)):\n",
    "    target_class = targets[i].item()\n",
    "    prob_of_true_class = probabilities[i, target_class].item()\n",
    "    neg_log_likelihood = -torch.log(probabilities[i, target_class]).item()\n",
    "    print(f\"Sample {i}: True class={target_class}, \"\n",
    "          f\"P(correct)={prob_of_true_class:.3f}, \"\n",
    "          f\"-log(P)={neg_log_likelihood:.3f}\")\n",
    "\n",
    "print(\"\\nüí° Training minimizes this loss = maximizes likelihood of correct predictions\")\n",
    "print(\"\\n‚Üí When P(correct) = 1.0, loss = 0 (perfect prediction)\")\n",
    "print(\"‚Üí When P(correct) = 0.5, loss = 0.69 (random guess for binary)\")\n",
    "print(\"‚Üí When P(correct) ‚Üí 0, loss ‚Üí ‚àû (completely wrong)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3B: Contrastive Learning (SimCLR-style)\n",
    "\n",
    "**Modern Context:** Contrastive learning methods like SimCLR, CLIP, and MoCo have revolutionized self-supervised learning. The core idea: maximize likelihood that similar items are close in embedding space, while dissimilar items are far apart.\n",
    "\n",
    "**Key Insight:** This is still maximum likelihood estimation, just with a cleverly designed likelihood function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ContrastiveModel(nn.Module):\n",
    "    \"\"\"Simple contrastive learning model (SimCLR-style)\"\"\"\n",
    "    def __init__(self, input_dim=20, embed_dim=8):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embeddings = self.encoder(x)\n",
    "        # L2 normalize embeddings\n",
    "        return F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "def contrastive_loss(embeddings, labels, temperature=0.5):\n",
    "    \"\"\"InfoNCE loss: maximize likelihood of correct pairs\"\"\"\n",
    "    # Compute similarity matrix\n",
    "    similarity = embeddings @ embeddings.T / temperature\n",
    "    \n",
    "    # Mask out self-similarity\n",
    "    batch_size = embeddings.shape[0]\n",
    "    mask = torch.eye(batch_size, dtype=torch.bool)\n",
    "    similarity = similarity.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    # Create targets: samples with same label should be close\n",
    "    labels = labels.unsqueeze(0)\n",
    "    positive_mask = (labels == labels.T).float()\n",
    "    positive_mask = positive_mask.masked_fill(mask, 0)\n",
    "    \n",
    "    # Compute loss (simplified version)\n",
    "    exp_sim = torch.exp(similarity)\n",
    "    log_prob = similarity - torch.log(exp_sim.sum(dim=1, keepdim=True))\n",
    "    loss = -(log_prob * positive_mask).sum() / positive_mask.sum()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Generate synthetic data\n",
    "X_contrast, y_contrast = make_classification(\n",
    "    n_samples=200, n_features=20, n_informative=15, \n",
    "    n_classes=4, random_state=42\n",
    ")\n",
    "X_tensor = torch.FloatTensor(X_contrast)\n",
    "y_tensor = torch.LongTensor(y_contrast)\n",
    "\n",
    "# Train contrastive model\n",
    "model = ContrastiveModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Contrastive Learning: Likelihood of Similar Pairs\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTraining...\")\n",
    "\n",
    "losses = []\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    embeddings = model(X_tensor)\n",
    "    loss = contrastive_loss(embeddings, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\n‚Üí The model learned to make similar items (same class) close in embedding space\")\n",
    "print(\"‚Üí This IS maximum likelihood estimation with a contrastive objective!\")\n",
    "print(\"\\nüí° Used in: CLIP (OpenAI), SimCLR (Google), MoCo (Facebook)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pillar 4: INTERCOMPARISON\n",
    "\n",
    "### üéØ Core Concept: Data Validates Itself\n",
    "\n",
    "**Historical Context:** The revolutionary realization that you don't need an external \"gold standard\" to assess statistical significance. The data can validate itself through techniques like Student's t-test, ANOVA, and confidence intervals.\n",
    "\n",
    "**Statistical Insight:** By comparing different parts of the data to each other, we can determine what's signal vs noise without external validation.\n",
    "\n",
    "**Modern ML Translation:**\n",
    "- **Cross-Validation:** Split data into K folds, train on K-1, test on 1\n",
    "- **Self-Supervised Learning:** Hide parts of data, predict from visible parts\n",
    "- **Masked Language Modeling (BERT):** Predict masked words from context\n",
    "- **Next Token Prediction (GPT):** Predict next word from previous words\n",
    "\n",
    "### Example 4A: K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Intercomparison: Cross-Validation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_cv, y_cv = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "model_cv = SGDClassifier(max_iter=1000, random_state=42)\n",
    "\n",
    "# 5-Fold Cross Validation\n",
    "cv_scores = cross_val_score(model_cv, X_cv, y_cv, cv=5)\n",
    "\n",
    "print(\"\\n5-Fold Cross-Validation Scores:\")\n",
    "for i, score in enumerate(cv_scores, 1):\n",
    "    print(f\"  Fold {i}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nMean Accuracy: {cv_scores.mean():.4f}\")\n",
    "print(f\"Standard Deviation: {cv_scores.std():.4f}\")\n",
    "print(f\"95% Confidence Interval: {cv_scores.mean():.4f} ¬± {1.96 * cv_scores.std():.4f}\")\n",
    "\n",
    "print(\"\\n‚Üí We assessed model performance without any external validation set\")\n",
    "print(\"‚Üí The data validated itself by comparing different subsets\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize cross-validation\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.bar(range(1, 6), cv_scores, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "plt.axhline(cv_scores.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {cv_scores.mean():.4f}')\n",
    "plt.axhline(cv_scores.mean() + cv_scores.std(), color='orange', linestyle=':', linewidth=1.5, label='¬±1 std')\n",
    "plt.axhline(cv_scores.mean() - cv_scores.std(), color='orange', linestyle=':', linewidth=1.5)\n",
    "\n",
    "plt.xlabel('Fold', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('5-Fold Cross-Validation Results\\nData Validates Itself', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4B: Self-Supervised Learning (Masked Prediction)\n",
    "\n",
    "**Modern Context:** BERT, MAE (Masked Autoencoders), and similar models use self-supervision. They hide parts of the input and train the model to predict the hidden parts from the visible parts.\n",
    "\n",
    "**Key Insight:** This is intercomparison at its finest‚Äîno human labels needed! The data provides its own supervision signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MaskedAutoencoderSimple(nn.Module):\n",
    "    \"\"\"Simplified masked autoencoder (MAE-style)\"\"\"\n",
    "    def __init__(self, input_dim=20, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        # Apply mask (zero out some features)\n",
    "        x_masked = x * mask\n",
    "        # Encode\n",
    "        encoded = self.encoder(x_masked)\n",
    "        # Decode\n",
    "        reconstructed = self.decoder(encoded)\n",
    "        return reconstructed\n",
    "\n",
    "# Generate data\n",
    "X_mae, _ = make_classification(n_samples=500, n_features=20, random_state=42)\n",
    "X_mae_tensor = torch.FloatTensor(X_mae)\n",
    "X_mae_tensor = (X_mae_tensor - X_mae_tensor.mean(0)) / (X_mae_tensor.std(0) + 1e-8)\n",
    "\n",
    "# Train masked autoencoder\n",
    "mae_model = MaskedAutoencoderSimple()\n",
    "optimizer_mae = optim.Adam(mae_model.parameters(), lr=0.01)\n",
    "mask_ratio = 0.5  # Mask 50% of features\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Self-Supervised Learning: Masked Autoencoding\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTraining model to predict {mask_ratio*100:.0f}% masked features from visible ones...\\n\")\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Random mask (different for each epoch)\n",
    "    mask = (torch.rand_like(X_mae_tensor) > mask_ratio).float()\n",
    "    \n",
    "    optimizer_mae.zero_grad()\n",
    "    reconstructed = mae_model(X_mae_tensor, mask)\n",
    "    \n",
    "    # Loss only on masked features\n",
    "    loss = F.mse_loss(reconstructed * (1 - mask), X_mae_tensor * (1 - mask))\n",
    "    loss.backward()\n",
    "    optimizer_mae.step()\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}: Reconstruction Loss = {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\n‚Üí Model learned to predict masked features from visible ones\")\n",
    "print(\"‚Üí No human labels needed‚Äîthe data supervises itself!\")\n",
    "print(\"\\nüí° This is how BERT, GPT, and vision transformers (MAE) are pretrained\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pillar 5: REGRESSION\n",
    "\n",
    "### üéØ Core Concept: Regression to the Mean\n",
    "\n",
    "**Historical Context:** Francis Galton's 1886 discovery studying heights of parents and children. Very tall parents tend to have shorter children (still tall, but less extreme). Very short parents tend to have taller children (still short, but less extreme). Nature regresses toward the average.\n",
    "\n",
    "**Statistical Insight:** Extreme events are likely followed by less extreme ones. This isn't mystical‚Äîit's the statistical reality that extreme values often involve luck/noise, which doesn't persist.\n",
    "\n",
    "**Modern ML Translation:**\n",
    "- **Regularization:** Preventing models from chasing extreme patterns (overfitting)\n",
    "- **L1/L2 Penalties:** Force weights toward zero (the \"mean\" of weight space)\n",
    "- **Dropout:** Randomly ignore neurons (regression to ensemble average)\n",
    "- **Early Stopping:** Stop before memorizing extreme training patterns\n",
    "- **Batch Normalization:** Normalize activations toward zero mean\n",
    "\n",
    "### Example 5A: Polynomial Regression with and without Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Regression to the Mean: Regularization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate noisy data with sinusoidal trend\n",
    "np.random.seed(42)\n",
    "X_reg = np.sort(np.random.rand(20, 1) * 10, axis=0)\n",
    "y_reg = np.sin(X_reg).ravel() + np.random.normal(0, 0.5, X_reg.shape[0])\n",
    "\n",
    "# 1. High-degree polynomial (will overfit)\n",
    "poly = PolynomialFeatures(degree=15)\n",
    "model_overfit = make_pipeline(poly, LinearRegression())\n",
    "model_overfit.fit(X_reg, y_reg)\n",
    "\n",
    "# 2. Regularized model (Ridge regression)\n",
    "model_regularized = make_pipeline(poly, Ridge(alpha=10.0))\n",
    "model_regularized.fit(X_reg, y_reg)\n",
    "\n",
    "# Test on dense grid\n",
    "X_test_reg = np.linspace(0, 10, 200)[:, np.newaxis]\n",
    "y_overfit = model_overfit.predict(X_test_reg)\n",
    "y_regularized = model_regularized.predict(X_test_reg)\n",
    "\n",
    "# Get coefficient magnitudes\n",
    "coef_overfit = model_overfit.named_steps['linearregression'].coef_\n",
    "coef_regularized = model_regularized.named_steps['ridge'].coef_\n",
    "\n",
    "print(f\"\\nDegree 15 Polynomial Fit:\")\n",
    "print(f\"  Overfit model (no regularization):\")\n",
    "print(f\"    Sum of |coefficients|: {np.sum(np.abs(coef_overfit)):.2f}\")\n",
    "print(f\"    Max |coefficient|: {np.max(np.abs(coef_overfit)):.2f}\")\n",
    "print(f\"\\n  Regularized model (Ridge):\")\n",
    "print(f\"    Sum of |coefficients|: {np.sum(np.abs(coef_regularized)):.2f}\")\n",
    "print(f\"    Max |coefficient|: {np.max(np.abs(coef_regularized)):.2f}\")\n",
    "\n",
    "print(f\"\\n‚Üí Regularization forces coefficients toward zero (regression to the mean)\")\n",
    "print(f\"‚Üí This prevents the model from chasing extreme patterns in training data\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize overfitting vs regularization\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_reg, y_reg, color='black', s=80, label='Training data', zorder=5)\n",
    "plt.plot(X_test_reg, y_overfit, 'r-', linewidth=2, label='Overfit (no regularization)')\n",
    "plt.plot(X_test_reg, np.sin(X_test_reg), 'g--', linewidth=2, alpha=0.5, label='True function')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Overfitting: Chasing Every Data Point\\n(High Variance)', fontsize=13)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(-2.5, 2.5)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_reg, y_reg, color='black', s=80, label='Training data', zorder=5)\n",
    "plt.plot(X_test_reg, y_regularized, 'b-', linewidth=2, label='Regularized (Ridge)')\n",
    "plt.plot(X_test_reg, np.sin(X_test_reg), 'g--', linewidth=2, alpha=0.5, label='True function')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Regularization: Smooth, General Pattern\\n(Regression to Mean)', fontsize=13)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(-2.5, 2.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Left plot: Model memorizes noise (chases extreme values)\")\n",
    "print(\"üí° Right plot: Regularization forces model toward simpler, more general pattern\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5B: Early Stopping - Temporal Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Early Stopping: Preventing Overfitting Through Time\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate data\n",
    "X_early, y_early = make_classification(n_samples=300, n_features=20, n_informative=10, random_state=42)\n",
    "X_train_e, X_val_e, y_train_e, y_val_e = train_test_split(X_early, y_early, test_size=0.33)\n",
    "\n",
    "X_train_e_t = torch.FloatTensor(X_train_e)\n",
    "y_train_e_t = torch.LongTensor(y_train_e)\n",
    "X_val_e_t = torch.FloatTensor(X_val_e)\n",
    "y_val_e_t = torch.LongTensor(y_val_e)\n",
    "\n",
    "# Create a model prone to overfitting\n",
    "class OverfitProneNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(20, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "net = OverfitProneNet()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "print(\"\\nTraining for 200 epochs and tracking validation performance...\\n\")\n",
    "\n",
    "for epoch in range(200):\n",
    "    # Training\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_out = net(X_train_e_t)\n",
    "    train_loss = F.cross_entropy(train_out, y_train_e_t)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(train_loss.item())\n",
    "    \n",
    "    # Validation\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_out = net(X_val_e_t)\n",
    "        val_loss = F.cross_entropy(val_out, y_val_e_t)\n",
    "        val_acc = (val_out.argmax(dim=1) == y_val_e_t).float().mean().item()\n",
    "        val_losses.append(val_loss.item())\n",
    "        val_accs.append(val_acc)\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}: Train Loss={train_loss.item():.4f}, \"\n",
    "              f\"Val Loss={val_loss.item():.4f}, Val Acc={val_acc:.4f}\")\n",
    "\n",
    "# Find optimal stopping point\n",
    "best_epoch = np.argmin(val_losses)\n",
    "print(f\"\\n‚Üí Best validation loss at epoch {best_epoch + 1}\")\n",
    "print(f\"‚Üí After that, model starts memorizing training noise (overfitting)\")\n",
    "print(f\"\\nüí° Early stopping = Temporal regularization (stop before going to extremes)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize early stopping\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_losses, label='Validation Loss', linewidth=2)\n",
    "plt.axvline(best_epoch, color='red', linestyle='--', linewidth=2, label=f'Optimal Stop (epoch {best_epoch+1})')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Early Stopping: Train vs Validation Loss', fontsize=13)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_accs, 'g-', linewidth=2)\n",
    "plt.axvline(best_epoch, color='red', linestyle='--', linewidth=2, label=f'Optimal Stop (epoch {best_epoch+1})')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Validation Accuracy', fontsize=12)\n",
    "plt.title('Validation Accuracy Over Time', fontsize=13)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pillar 6: DESIGN\n",
    "\n",
    "### üéØ Core Concept: How You Collect Data Matters\n",
    "\n",
    "**Historical Context:** Ronald Fisher's revolutionary insight in the 1920s: **randomization** in experimental design eliminates confounding variables. How you collect and present data is more important than how you analyze it.\n",
    "\n",
    "**Statistical Insight:** Biased data collection ‚Üí biased conclusions, no matter how sophisticated your analysis. Randomization ensures treatment groups are comparable.\n",
    "\n",
    "**Modern ML Translation:**\n",
    "- **Stochastic Gradient Descent (SGD):** \"Stochastic\" = random. Shuffling batches is critical!\n",
    "- **Data Augmentation:** Creating diverse training examples\n",
    "- **Curriculum Learning:** Strategic ordering of training data (easy ‚Üí hard)\n",
    "- **Active Learning:** Strategically selecting which examples to label\n",
    "\n",
    "### Example 6A: SGD with vs without Randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Design: The Critical Role of Randomization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create dataset sorted by class (all 0s, then all 1s)\n",
    "X_design, y_design = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "sorted_indices = np.argsort(y_design)\n",
    "X_sorted, y_sorted = X_design[sorted_indices], y_design[sorted_indices]\n",
    "\n",
    "print(\"\\nDataset structure:\")\n",
    "print(f\"  First 500 samples: all class {y_sorted[0]}\")\n",
    "print(f\"  Last 500 samples: all class {y_sorted[-1]}\")\n",
    "print(\"\\nTraining two models with same data, different presentation order...\\n\")\n",
    "\n",
    "# Model 1: Sequential batches (BAD DESIGN - no randomization)\n",
    "clf_sequential = SGDClassifier(shuffle=False, max_iter=1, warm_start=True, random_state=42)\n",
    "\n",
    "# Model 2: Random batches (GOOD DESIGN - randomization)\n",
    "clf_random = SGDClassifier(shuffle=True, max_iter=1, warm_start=True, random_state=42)\n",
    "\n",
    "seq_accs = []\n",
    "rand_accs = []\n",
    "\n",
    "# Simulate mini-batch training\n",
    "for iteration in range(10):\n",
    "    for i in range(0, 1000, 100):\n",
    "        # Sequential: Process data in order (all 0s first, then all 1s)\n",
    "        batch_X_seq = X_sorted[i:i+100]\n",
    "        batch_y_seq = y_sorted[i:i+100]\n",
    "        clf_sequential.partial_fit(batch_X_seq, batch_y_seq, classes=[0, 1])\n",
    "        \n",
    "        # Random: Process random batches\n",
    "        rand_idx = np.random.randint(0, 1000, 100)\n",
    "        batch_X_rand = X_sorted[rand_idx]\n",
    "        batch_y_rand = y_sorted[rand_idx]\n",
    "        clf_random.partial_fit(batch_X_rand, batch_y_rand, classes=[0, 1])\n",
    "    \n",
    "    seq_acc = clf_sequential.score(X_sorted, y_sorted)\n",
    "    rand_acc = clf_random.score(X_sorted, y_sorted)\n",
    "    seq_accs.append(seq_acc)\n",
    "    rand_accs.append(rand_acc)\n",
    "    \n",
    "    print(f\"Iteration {iteration+1:2d}: Sequential={seq_acc:.4f}, Randomized={rand_acc:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Final Performance:\")\n",
    "print(f\"  Sequential (BAD design): {seq_accs[-1]:.4f}\")\n",
    "print(f\"  Randomized (GOOD design): {rand_accs[-1]:.4f}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"\\n‚Üí Without randomization, the model 'forgets' early classes as it sees later ones\")\n",
    "print(\"‚Üí Randomization ensures balanced exposure to all patterns\")\n",
    "print(\"\\nüí° This is why 'shuffle=True' is default in PyTorch DataLoader!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6B: Curriculum Learning (Easy ‚Üí Hard)\n",
    "\n",
    "**Modern Context:** While randomization is critical, research shows that strategic ordering can sometimes help. **Curriculum learning** presents easy examples first, then gradually increases difficulty‚Äîlike how humans learn.\n",
    "\n",
    "**Key Insight:** This isn't anti-randomization; it's thoughtful design of the training sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Curriculum Learning: Strategic Data Ordering\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate data with varying difficulty\n",
    "X_curr, y_curr = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_curr, y_curr, test_size=0.3)\n",
    "\n",
    "# Define \"difficulty\" as distance from decision boundary\n",
    "# Train a simple model to get decision boundary\n",
    "temp_model = LogisticRegression()\n",
    "temp_model.fit(X_train_c, y_train_c)\n",
    "decision_values = np.abs(temp_model.decision_function(X_train_c))\n",
    "\n",
    "# Sort by difficulty (high decision value = easy/confident, low = hard/ambiguous)\n",
    "difficulty_order = np.argsort(decision_values)[::-1]  # Easy to hard\n",
    "\n",
    "# Prepare tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_c)\n",
    "y_train_tensor = torch.LongTensor(y_train_c)\n",
    "X_test_tensor = torch.FloatTensor(X_test_c)\n",
    "y_test_tensor = torch.LongTensor(y_test_c)\n",
    "\n",
    "# Two identical networks\n",
    "net_curriculum = nn.Sequential(\n",
    "    nn.Linear(2, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 2)\n",
    ")\n",
    "\n",
    "net_random = nn.Sequential(\n",
    "    nn.Linear(2, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 2)\n",
    ")\n",
    "\n",
    "optimizer_curr = optim.SGD(net_curriculum.parameters(), lr=0.01)\n",
    "optimizer_rand = optim.SGD(net_random.parameters(), lr=0.01)\n",
    "\n",
    "print(\"\\nTraining two identical networks:\")\n",
    "print(\"  1. Curriculum: Easy examples ‚Üí Hard examples\")\n",
    "print(\"  2. Random order\\n\")\n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Curriculum training\n",
    "    if epoch < 25:  # First half: only easy examples\n",
    "        curriculum_idx = difficulty_order[:len(difficulty_order)//2]\n",
    "    else:  # Second half: all examples\n",
    "        curriculum_idx = difficulty_order\n",
    "    \n",
    "    for i in range(0, len(curriculum_idx), batch_size):\n",
    "        batch_idx = curriculum_idx[np.random.choice(len(curriculum_idx), \n",
    "                                                     min(batch_size, len(curriculum_idx)))]\n",
    "        optimizer_curr.zero_grad()\n",
    "        outputs = net_curriculum(X_train_tensor[batch_idx])\n",
    "        loss = F.cross_entropy(outputs, y_train_tensor[batch_idx])\n",
    "        loss.backward()\n",
    "        optimizer_curr.step()\n",
    "    \n",
    "    # Random order training\n",
    "    random_idx = np.random.permutation(len(X_train_c))\n",
    "    for i in range(0, len(X_train_c), batch_size):\n",
    "        batch_idx = random_idx[i:i+batch_size]\n",
    "        optimizer_rand.zero_grad()\n",
    "        outputs = net_random(X_train_tensor[batch_idx])\n",
    "        loss = F.cross_entropy(outputs, y_train_tensor[batch_idx])\n",
    "        loss.backward()\n",
    "        optimizer_rand.step()\n",
    "\n",
    "# Evaluate\n",
    "net_curriculum.eval()\n",
    "net_random.eval()\n",
    "with torch.no_grad():\n",
    "    curr_pred = net_curriculum(X_test_tensor).argmax(dim=1)\n",
    "    rand_pred = net_random(X_test_tensor).argmax(dim=1)\n",
    "    \n",
    "    curr_acc = (curr_pred == y_test_tensor).float().mean().item()\n",
    "    rand_acc = (rand_pred == y_test_tensor).float().mean().item()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Final Test Accuracy:\")\n",
    "print(f\"  Random order: {rand_acc:.4f}\")\n",
    "print(f\"  Curriculum (easy‚Üíhard): {curr_acc:.4f}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"\\n‚Üí Strategic ordering can improve learning efficiency and generalization\")\n",
    "print(\"‚Üí Design of training sequence matters!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pillar 7: RESIDUAL\n",
    "\n",
    "### üéØ Core Concept: Structure in What's Left Over\n",
    "\n",
    "**Historical Context:** After fitting a model, examine the residuals (observed - predicted). If residuals look like random noise, you're done. If there's structure in the residuals, your model missed something important.\n",
    "\n",
    "**Statistical Insight:** Residuals = Data - Model. They tell you what your model doesn't understand yet. Systematic patterns in residuals indicate missing features or wrong functional form.\n",
    "\n",
    "**Modern ML Translation:**\n",
    "- **Gradient Boosting:** Explicitly trains new models on residuals of previous models\n",
    "- **ResNet Architecture:** Skip connections let layers learn residual mappings\n",
    "- **Error Analysis:** Examining which examples the model gets wrong\n",
    "- **Hard Negative Mining:** Focus training on difficult/misclassified examples\n",
    "\n",
    "### Example 7A: Gradient Boosting - Iterative Residual Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Residual: Learning from What's Left Over\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_res, y_res = make_regression(n_samples=200, n_features=10, noise=10, random_state=42)\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_res, y_res, random_state=42)\n",
    "\n",
    "# Gradient Boosting: Each tree learns residuals from previous trees\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, \n",
    "                                 max_depth=3, random_state=42)\n",
    "gb.fit(X_train_r, y_train_r)\n",
    "\n",
    "train_errors = gb.train_score_  # MSE at each boosting iteration\n",
    "\n",
    "print(\"\\nGradient Boosting Training Progress:\")\n",
    "print(f\"  Initial error (after tree 1): {train_errors[0]:.2f}\")\n",
    "print(f\"  After 25 trees: {train_errors[24]:.2f}\")\n",
    "print(f\"  After 50 trees: {train_errors[49]:.2f}\")\n",
    "print(f\"  After 100 trees: {train_errors[-1]:.2f}\")\n",
    "print(f\"\\nTest R¬≤ Score: {gb.score(X_test_r, y_test_r):.4f}\")\n",
    "\n",
    "print(\"\\n‚Üí Each new tree in the ensemble learns from the residuals (errors) of all previous trees\")\n",
    "print(\"‚Üí This iteratively discovers structure hidden in what earlier models missed\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize boosting progression\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, 101), train_errors, 'b-', linewidth=2)\n",
    "plt.xlabel('Number of Trees', fontsize=12)\n",
    "plt.ylabel('Training MSE', fontsize=12)\n",
    "plt.title('Gradient Boosting: Iterative Error Reduction\\nEach Tree Learns From Residuals', fontsize=13)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show staged predictions (predictions at different stages of boosting)\n",
    "stages = [1, 10, 25, 50, 100]\n",
    "plt.subplot(1, 2, 2)\n",
    "for stage in stages:\n",
    "    gb_temp = GradientBoostingRegressor(n_estimators=stage, learning_rate=0.1,\n",
    "                                         max_depth=3, random_state=42)\n",
    "    gb_temp.fit(X_train_r, y_train_r)\n",
    "    y_pred = gb_temp.predict(X_test_r)\n",
    "    plt.scatter(y_test_r, y_pred, alpha=0.5, label=f'{stage} trees', s=30)\n",
    "\n",
    "plt.plot([y_test_r.min(), y_test_r.max()], [y_test_r.min(), y_test_r.max()], \n",
    "         'r--', linewidth=2, label='Perfect prediction')\n",
    "plt.xlabel('True Values', fontsize=12)\n",
    "plt.ylabel('Predicted Values', fontsize=12)\n",
    "plt.title('Predictions Improve as More Trees\\nLearn From Residuals', fontsize=13)\n",
    "plt.legend(fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7B: ResNet - Residual Learning in Architecture\n",
    "\n",
    "**Modern Context:** ResNet (2015) revolutionized deep learning by adding **skip connections**. Instead of learning the full mapping H(x), layers learn the residual F(x) = H(x) - x. This enabled networks with 100+ layers (vs ~20 before ResNet).\n",
    "\n",
    "**Key Insight:** The \"residual\" concept from statistics became an architectural principle. Learn what to **add** to the input, not the full transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ResNet: Residual Learning in Deep Network Architecture\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Block with skip connection: output = F(x) + x\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels)\n",
    "        self.fc2 = nn.Linear(channels, channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x  # Save input\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(out)  # Learn F(x)\n",
    "        out = out + residual  # Add skip connection: F(x) + x\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class PlainBlock(nn.Module):\n",
    "    \"\"\"Block without skip connection: output = F(x)\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels)\n",
    "        self.fc2 = nn.Linear(channels, channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(out)  # Learn H(x) directly\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# Deep networks with and without residual connections\n",
    "class DeepResNet(nn.Module):\n",
    "    def __init__(self, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(20, 64)\n",
    "        self.blocks = nn.ModuleList([ResidualBlock(64) for _ in range(n_blocks)])\n",
    "        self.output_layer = nn.Linear(64, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "class DeepPlainNet(nn.Module):\n",
    "    def __init__(self, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(20, 64)\n",
    "        self.blocks = nn.ModuleList([PlainBlock(64) for _ in range(n_blocks)])\n",
    "        self.output_layer = nn.Linear(64, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Generate data\n",
    "X_resnet, y_resnet = make_classification(n_samples=500, n_features=20, random_state=42)\n",
    "X_train_rn, X_test_rn, y_train_rn, y_test_rn = train_test_split(X_resnet, y_resnet)\n",
    "\n",
    "X_train_rn_t = torch.FloatTensor(X_train_rn)\n",
    "y_train_rn_t = torch.LongTensor(y_train_rn)\n",
    "X_test_rn_t = torch.FloatTensor(X_test_rn)\n",
    "y_test_rn_t = torch.LongTensor(y_test_rn)\n",
    "\n",
    "print(\"\\nTraining deep networks (10 layers):\")\n",
    "print(\"  1. Plain network (no skip connections)\")\n",
    "print(\"  2. ResNet (with skip connections)\\n\")\n",
    "\n",
    "# Train both networks\n",
    "resnet = DeepResNet(n_blocks=10)\n",
    "plainnet = DeepPlainNet(n_blocks=10)\n",
    "\n",
    "optimizer_res = optim.Adam(resnet.parameters(), lr=0.001)\n",
    "optimizer_plain = optim.Adam(plainnet.parameters(), lr=0.001)\n",
    "\n",
    "resnet_losses = []\n",
    "plainnet_losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Train ResNet\n",
    "    resnet.train()\n",
    "    optimizer_res.zero_grad()\n",
    "    res_out = resnet(X_train_rn_t)\n",
    "    res_loss = F.cross_entropy(res_out, y_train_rn_t)\n",
    "    res_loss.backward()\n",
    "    optimizer_res.step()\n",
    "    resnet_losses.append(res_loss.item())\n",
    "    \n",
    "    # Train PlainNet\n",
    "    plainnet.train()\n",
    "    optimizer_plain.zero_grad()\n",
    "    plain_out = plainnet(X_train_rn_t)\n",
    "    plain_loss = F.cross_entropy(plain_out, y_train_rn_t)\n",
    "    plain_loss.backward()\n",
    "    optimizer_plain.step()\n",
    "    plainnet_losses.append(plain_loss.item())\n",
    "\n",
    "# Final evaluation\n",
    "resnet.eval()\n",
    "plainnet.eval()\n",
    "with torch.no_grad():\n",
    "    res_pred = resnet(X_test_rn_t).argmax(dim=1)\n",
    "    plain_pred = plainnet(X_test_rn_t).argmax(dim=1)\n",
    "    \n",
    "    res_acc = (res_pred == y_test_rn_t).float().mean().item()\n",
    "    plain_acc = (plain_pred == y_test_rn_t).float().mean().item()\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Final Results:\")\n",
    "print(f\"\\nPlain Network (no skip connections):\")\n",
    "print(f\"  Training loss: {plainnet_losses[-1]:.4f}\")\n",
    "print(f\"  Test accuracy: {plain_acc:.4f}\")\n",
    "print(f\"\\nResNet (with skip connections):\")\n",
    "print(f\"  Training loss: {resnet_losses[-1]:.4f}\")\n",
    "print(f\"  Test accuracy: {res_acc:.4f}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(\"\\n‚Üí Skip connections allow gradients to flow directly through the network\")\n",
    "print(\"‚Üí Each layer learns the residual (what to add) rather than full mapping\")\n",
    "print(\"‚Üí This enabled networks with 100+ layers (vs ~20 max before ResNet)\")\n",
    "print(\"\\nüí° ResNet won ImageNet 2015 and revolutionized deep learning architecture\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(resnet_losses, label='ResNet (with skip connections)', linewidth=2)\n",
    "plt.plot(plainnet_losses, label='Plain Network (no skip connections)', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Training Loss', fontsize=12)\n",
    "plt.title('Training Loss: ResNet vs Plain Network', fontsize=13)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Show the difference\n",
    "improvement = np.array(plainnet_losses) - np.array(resnet_losses)\n",
    "plt.plot(improvement, 'g-', linewidth=2)\n",
    "plt.axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss Difference\\n(Plain - ResNet)', fontsize=12)\n",
    "plt.title('ResNet Advantage Over Training', fontsize=13)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Positive values = ResNet is training better (lower loss)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Synthesis: How the Pillars Work Together\n",
    "\n",
    "### Training a Modern LLM Touches All 7 Pillars\n",
    "\n",
    "Let's see how **all seven pillars** appear in a single modern system‚Äîtraining a large language model:\n",
    "\n",
    "#### 1. **AGGREGATION**\n",
    "- Mixture of Experts architectures (GPT-4, Mixtral)\n",
    "- Multiple attention heads aggregate different representations\n",
    "- Ensemble of models for final deployment\n",
    "\n",
    "#### 2. **INFORMATION**\n",
    "- Scaling laws determine data/compute tradeoffs\n",
    "- Understanding that 10√ó more data gives only ~2√ó better performance\n",
    "- Data curation and deduplication to maximize information density\n",
    "\n",
    "#### 3. **LIKELIHOOD**\n",
    "- Cross-entropy loss minimization (next token prediction)\n",
    "- Maximizing likelihood of training data\n",
    "- Softmax converts logits to probability distributions\n",
    "\n",
    "#### 4. **INTERCOMPARISON**\n",
    "- Self-supervised pretraining (no human labels needed!)\n",
    "- Next token prediction: predict from context\n",
    "- Validation sets carved from training data\n",
    "\n",
    "#### 5. **REGRESSION**\n",
    "- Weight decay (L2 regularization)\n",
    "- Dropout in training\n",
    "- Layer normalization\n",
    "- Early stopping based on validation loss\n",
    "\n",
    "#### 6. **DESIGN**\n",
    "- Stochastic gradient descent with careful batch sampling\n",
    "- Data shuffling and curriculum strategies\n",
    "- Careful curation of pretraining corpus\n",
    "- Temperature settings in sampling\n",
    "\n",
    "#### 7. **RESIDUAL**\n",
    "- Residual connections in every transformer block\n",
    "- Error analysis of model failures\n",
    "- RLHF learns from preference residuals\n",
    "- Iterative refinement based on what model gets wrong\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "**These seven pillars aren't separate techniques‚Äîthey're different facets of the same underlying statistical philosophy that Stigler identified in classical statistics and that we've reinvented in the age of deep learning.**\n",
    "\n",
    "The \"new\" AI revolution is built on very old statistical wisdom. Understanding these foundations helps us:\n",
    "- Design better models\n",
    "- Diagnose failures more effectively  \n",
    "- Appreciate that modern ML is statistics at scale\n",
    "- Predict what techniques will work and why\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading & Resources\n",
    "\n",
    "### Books\n",
    "- **Stigler, S.M.** (2016). *The Seven Pillars of Statistical Wisdom*. Harvard University Press.\n",
    "- **Efron, B. & Hastie, T.** (2016). *Computer Age Statistical Inference*. Cambridge University Press.\n",
    "- **Goodfellow, I., Bengio, Y., & Courville, A.** (2016). *Deep Learning*. MIT Press.\n",
    "- **Murphy, K.P.** (2022). *Probabilistic Machine Learning: An Introduction*. MIT Press.\n",
    "\n",
    "### Key Papers\n",
    "- **Breiman, L.** (2001). Random Forests. *Machine Learning*, 45(1), 5-32.\n",
    "- **He, K. et al.** (2016). Deep Residual Learning for Image Recognition. *CVPR*.\n",
    "- **Vaswani, A. et al.** (2017). Attention is All You Need. *NeurIPS*.\n",
    "- **Kaplan, J. et al.** (2020). Scaling Laws for Neural Language Models. *arXiv*.\n",
    "- **Hoffmann, J. et al.** (2022). Training Compute-Optimal Large Language Models. *arXiv*.\n",
    "\n",
    "### Models on Hugging Face\n",
    "- `bert-base-uncased` - Masked language modeling (Intercomparison)\n",
    "- `gpt2` - Next token prediction (Likelihood)\n",
    "- `openai/clip-vit-base-patch32` - Contrastive learning\n",
    "- `microsoft/resnet-50` - Residual learning architecture\n",
    "\n",
    "### Interactive Resources\n",
    "- **Distill.pub** - Visual explanations of ML concepts\n",
    "- **Jay Alammar's Blog** - Illustrated transformer, BERT, GPT\n",
    "- **3Blue1Brown** - Neural networks video series\n",
    "- **Fast.ai Course** - Practical deep learning\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook created as part of \"The Seven Pillars of Statistical Wisdom in Modern AI/ML\" project.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
